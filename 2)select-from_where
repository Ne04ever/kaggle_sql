Introduction


 SELECT, FROM, and WHERE 

For clarity, we'll work with a small imaginary dataset pet_records which contains just one table, called pets.



The most basic SQL query selects a single column from a single table. To do this,

    specify the column you want after the word SELECT, and then
    specify the table after the word FROM.



Note that when writing an SQL query, the argument we pass to FROM is not in single or double quotation marks (' or "). It is in backticks (`).
BigQuery datasets are large, so you'll usually want to return only the rows meeting specific conditions. You can do this using the WHERE clause.


code

query = """
        SELECT Name
        FROM `bigquery-public-data.pet_reocrds.pets`
        WHERE country = 'cat'
        """
        
        
        
        
        





Now that you've got the basics down, let's work through an example with a real dataset. We'll use an OpenAQ dataset about air quality.




from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "openaq" dataset
dataset_ref = client.dataset("openaq", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "openaq" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there's only one!)
for table in tables:  
    print(table.table_id)


Using Kaggle's public dataset BigQuery integration.
global_air_quality

The dataset contains only one table, called global_air_quality.



# Construct a reference to the "global_air_quality" table
table_ref = dataset_ref.table("global_air_quality")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "global_air_quality" table
client.list_rows(table, max_results=5).to_dataframe()

	location 	city 	country 	pollutant 	value 	timestamp 	unit 	source_name 	latitude 	longitude 	averaged_over_in_hours
0 	BTM Layout, Bengaluru - KSPCB 	Bengaluru 	IN 	co 	910.00 	2018-02-22 03:00:00+00:00 	µg/m³ 	CPCB 	12.912811 	77.60922 	0.25
1 	BTM Layout, Bengaluru - KSPCB 	Bengaluru 	IN 	no2 	131.87 	2018-02-22 03:00:00+00:00 	µg/m³ 	CPCB 	12.912811 	77.60922 	0.25
2 	BTM Layout, Bengaluru - KSPCB 	Bengaluru 	IN 	o3 	15.57 	2018-02-22 03:00:00+00:00 	µg/m³ 	CPCB 	12.912811 	77.60922 	0.25
3 	BTM Layout, Bengaluru - KSPCB 	Bengaluru 	IN 	pm25 	45.62 	2018-02-22 03:00:00+00:00 	µg/m³ 	CPCB 	12.912811 	77.60922 	0.25
4 	BTM Layout, Bengaluru - KSPCB 	Bengaluru 	IN 	so2 	4.49 	2018-02-22 03:00:00+00:00 	µg/m³ 	CPCB 	12.912811 	77.60922 	0.25







Everything looks good! So, let's put together a query. Say we want to select all the values from the city column that are in rows where the country column is 'US' (for "United States").

# Query to select all the items from the "city" column where the "country" column is 'US'
query = """
        SELECT city
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """


We're ready to use this query to get information from the OpenAQ dataset. As in the previous tutorial, the first step is to create a Client object.

# Create a "Client" object
client = bigquery.Client()



We begin by setting up the query with the query() method. We run the method with the default parameters, but this method also allows us to specify more complicated settings that you can read about in the documentation. We'll revisit this later.

# Set up the query
query_job = client.query(query)

Next, we run the query and convert the results to a pandas DataFrame.

# API request - run the query, and return a pandas DataFrame
us_cities = query_job.to_dataframe()

Now we've got a pandas DataFrame called us_cities, which we can use like any other DataFrame.

# What five cities have the most measurements?
us_cities.city.value_counts().head()

Phoenix-Mesa-Scottsdale                     88
Houston                                     82
Los Angeles-Long Beach-Santa Ana            68
Riverside-San Bernardino-Ontario            60
New York-Northern New Jersey-Long Island    60
Name: city, dtype: int64

More queries

If you want multiple columns, you can select them with a comma between the names:

query = """
        SELECT city, country
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """

You can select all columns with a * like this:

query = """
        SELECT *
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """

Q&A: Notes on formatting

The formatting of the SQL query might feel unfamiliar. If you have any questions, you can ask in the comments section at the bottom of this page. Here are answers to two common questions:
Question: What's up with the triple quotation marks (""")?

Answer: These tell Python that everything inside them is a single string, even though we have line breaks in it. The line breaks aren't necessary, but they make it easier to read your query.
Question: Do you need to capitalize SELECT and FROM?

Answer: No, SQL doesn't care about capitalization. However, it's customary to capitalize your SQL commands, and it makes your queries a bit easier to read.
Working with big datasets









To begin,you can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True.

# Query to get the score column from every row where the type column has value "job"
query = """
        SELECT score, title
        FROM `bigquery-public-data.hacker_news.full`
        WHERE type = "job" 
        """

# Create a QueryJobConfig object to estimate size of query without running it
dry_run_config = bigquery.QueryJobConfig(dry_run=True)

# API request - dry run query to estimate costs
dry_run_query_job = client.query(query, job_config=dry_run_config)

print("This query will process {} bytes.".format(dry_run_query_job.total_bytes_processed))

This query will process 429036722 bytes.









You can also specify a parameter when running the query to limit how much data you are willing to scan. 

# Only run the query if it's less than 1 MB
ONE_MB = 1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)

# Set up the query (will only run if it's less than 1 MB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
safe_query_job.to_dataframe()

---------------------------------------------------------------------------

InternalServerError: 500 Query exceeded limit for bytes billed: 1000000. 429916160 or higher required.

In this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can increase the limit to run the query successfully!








# Only run the query if it's less than 1 GB
ONE_GB = 1000*1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)

# Set up the query (will only run if it's less than 1 GB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
job_post_scores = safe_query_job.to_dataframe()

# Print average score for job posts
job_post_scores.score.mean()







To get each country only once you can run `SELECT DISTINCT country ...`. The DISTINCT keyword ensures each column shows up once, which you'll want in some cases.



